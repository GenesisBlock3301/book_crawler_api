# Book Crawler API ğŸ“š

A **production-grade**, scalable web crawling and monitoring system for [books.toscrape.com](https://books.toscrape.com) built with **FastAPI**, **MongoDB**, **Redis**, and **APScheduler**.

---

## ğŸŒŸ Features

- âœ… **Async Web Crawler** using `aiohttp` and `selectolax`
- âœ… **MongoDB Storage** with deduplication and change tracking
- âœ… **Scheduled Change Detection** via APScheduler
- âœ… **RESTful API** with OpenAPI documentation
- âœ… **User Authentication** with API key-based access control
- âœ… **Rate Limiting** using Redis-backed `slowapi`
- âœ… **Admin/Main User** system for user management
- âœ… **Comprehensive Test Suite** with `pytest-asyncio`

---

## ğŸ“‹ Table of Contents

1. [Tech Stack](#-tech-stack)
2. [Project Structure](#-project-structure)
3. [Prerequisites](#-prerequisites)
4. [Installation & Setup](#-installation--setup)
5. [Running the Application](#-running-the-application)
6. [API Documentation](#-api-documentation)
7. [Testing](#-testing)
8. [Deployment](#-deployment)
9. [Troubleshooting](#-troubleshooting)
10. [Support & License](#-support--license)

---

## ğŸ›  Tech Stack

| Component                 | Technology             | Purpose                         |
|---------------------------|------------------------|---------------------------------|
| **Web Framework**         | FastAPI                | High-performance async REST API |
| **Database**              | MongoDB (Motor)        | NoSQL storage with async driver |
| **Cache & Rate Limiting** | Redis                  | Token bucket rate limiting      |
| **HTML Parsing**          | Selectolax             | Fast, memory-efficient parsing  |
| **HTTP Client**           | aiohttp                | Async web crawling              |
| **Scheduler**             | APScheduler            | Daily change detection jobs     |
| **Validation**            | Pydantic               | Data schema validation          |
| **Testing**               | pytest, pytest-asyncio | Async unit/integration tests    |

---

## ğŸ“ Project Structure

```
book_crawler_api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                    # FastAPI routes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ book_router.py      # GET /books, /books/{id}
â”‚   â”‚   â”œâ”€â”€ change_router.py    # GET /changes, /changes/{id}
â”‚   â”‚   â””â”€â”€ user_router.py      # POST /users (admin only)
â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ crawler.py          # BookCrawler class
â”‚   â”‚   â””â”€â”€ parser.py           # HTML parsing logic
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ repository
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ book_repository.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py
â”‚   â”‚   â”‚   â”œâ”€â”€ change_book_repo.py
â”‚   â”‚   â”‚   â””â”€â”€ user_repository.py
â”‚   â”‚   â””â”€â”€ database.py          # MongoDB connection
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ scheduler.py        # APScheduler setup
â”‚   â”‚   â””â”€â”€ detector.py         # Change detection logic
â”‚   â”œâ”€â”€ schemas/                # Pydantic models
â”‚   â”‚   â”œâ”€â”€ __init__.py 
â”‚   â”‚   â”œâ”€â”€ book_schemas.py
â”‚   â”‚   â””â”€â”€ user_schemas.py 
â”‚   â”œâ”€â”€ services/           
â”‚   â”‚   â”œâ”€â”€ __init__.py 
â”‚   â”‚   â”œâ”€â”€ book_service.py
â”‚   â”‚   â”œâ”€â”€ change_book_service.py
â”‚   â”‚   â””â”€â”€ user_service.py 
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ security.py         
â”‚   â”‚   â”œâ”€â”€ enums.py
â”‚   â”‚   â”œâ”€â”€ logger.py                
â”‚   â”‚   â””â”€â”€ pagination.py  
â”‚   â”œâ”€â”€ tests/                
â”‚   â”‚   â”œâ”€â”€ __init__.py 
â”‚   â”‚   â”œâ”€â”€ test_book_api.py
â”‚   â”‚   â”œâ”€â”€ test_change_book_api.py
â”‚   â”‚   â”œâ”€â”€ test_scheduler.py
â”‚   â”‚   â””â”€â”€ test_crawler.py 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py                 # FastAPI app entry point
â”œâ”€â”€ .env                        # Environment variables (DO NOT COMMIT)
â”œâ”€â”€ .env.example                # Template for .env
â”œâ”€â”€ docker-compose.yml          # MongoDB + Redis services
â”œâ”€â”€ Dockerfile                  # App containerization
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md

````

---

## âœ… Prerequisites

- **Python**: 3.10+ (tested on 3.13.5)
- **Docker & Docker Compose**: For MongoDB and Redis
- **Git**: For cloning the repository

---

## ğŸš€ Installation & Setup

### 1. Clone the Repository

```
git clone https://github.com/yourusername/book_crawler_api.git
cd book_crawler_api
````

### 2. Create Virtual Environment

```
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Configure Environment Variables

```
cp .env.example .env
```

Edit `.env` with your settings:

```
MONGO_URL=mongodb://localhost:27017
DB_NAME=bookstore
BASE_URL=https://books.toscrape.com
ADMIN_API_KEY=supersecretkey123
REDIS_URL=redis://localhost:6379
HOST=http://localhost:8000
```

Edit `.env` with your settings for containerization:
```
MONGO_URL=mongodb://mongo:27017
DB_NAME=bookstore
BASE_URL=https://books.toscrape.com
ADMIN_API_KEY=supersecretkey123
REDIS_URL=redis://redis:6379
HOST=http://localhost:8000
```

### 5. Start Infrastructure Services

```bash
docker-compose up -d
```

Verify services:

```bash
docker-compose ps
```

---

## ğŸ¯ Running the Application

**Always run commands from the project root** (`book_crawler_api/`).

### 1. Run the Crawler (Initial Setup)

```bash
python -m app.crawler.crawler
```

This will:

* Crawl all books from `books.toscrape.com`
* Store data in MongoDB (`books` collection)
* Save raw HTML snapshots

Expected output:

```
âœ“ Crawled 1000 books in 45.3s
âœ“ Stored in MongoDB: book_crawler_db.books
```

### 2. Start the FastAPI Server

```
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Or via Python module:

```
python -m app.main
```

API docs:

* **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)
* **ReDoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc)

### 3. Run the Scheduler (Change Detection)

```
python -m app.scheduler.scheduler
```

This runs daily at midnight to:

* Detect new books
* Track price/availability changes
* Log changes to `book_changes` collection

---

## ğŸ”‘ API Documentation

### Authentication

All endpoints require an **API key**:

```
-H "x-api-key: YOUR_API_KEY"
```

#### Admin: Create Users

The first admin user is auto-created from `.env`.

**Create a new user**:

```
POST /api/users
Content-Type: application/json
x-api-key: <ADMIN_API_KEY>

{
  "username": "developer"
}
```

Response:

```json
{
  "message": "User & API key created successfully",
  "user": {
    "username": "developer",
    "api_key": "a3f8c2e1d5b9f7a2c4e6d8f0b2a4c6e8",
    "is_active": true,
    "role": "user",
    "created_at": "2025-01-15T10:30:00Z"
  }
}
```

---

### Book Endpoints

#### 1. Get All Books

```
GET /api/books
```

**Query Parameters:**

| Parameter   | Type   | Description            | Example                      |
|-------------|--------|------------------------|------------------------------|
| `category`  | string | Filter by category     | `Travel`                     |
| `min_price` | float  | Minimum price          | `20.0`                       |
| `max_price` | float  | Maximum price          | `50.0`                       |
| `rating`    | int    | Filter by rating (1-5) | `4`                          |
| `sort_by`   | string | Sort field             | `price`, `rating`, `reviews` |
| `page`      | int    | Page number            | `1`                          |
| `limit`     | int    | Items per page         | `20`                         |

**Example Request:**

```
curl -X GET "http://localhost:8000/api/books?category=Travel&min_price=20&max_price=50&rating=4&sort_by=price&page=1&limit=10" \
  -H "x-api-key: YOUR_API_KEY"
```

**Response:**

```json
{
  "total": 47,
  "page": 1,
  "limit": 10,
  "results": [
    {
      "_id": "68ef592250ca2000ff19b001",
      "name": "A Light in the Attic",
      "description": "It's hard to imagine a world without...",
      "category": "Poetry",
      "price": {
        "including_tax": 51.77,
        "excluding_tax": 51.77
      },
      "availability": "In stock (22 available)",
      "num_reviews": 0,
      "image_url": "https://books.toscrape.com/media/cache/...",
      "rating": 3,
      "source_url": "https://books.toscrape.com/catalogue/...",
      "crawl_timestamp": "2025-01-15T08:00:00Z"
    }
  ]
}
```

#### 2. Get Single Book by ID

```
GET /api/books/{book_id}
```

---

### Change Tracking Endpoints

#### 1. Get All Changes

```
GET /api/changes
```

**Query Parameters:**

* `page` (int)
* `limit` (int)
* `change_type` (string): `price_change`, `new_book`, `availability_change`

#### 2. Get Change by ID

```
GET /api/changes/{change_id}
```

---

### Rate Limiting

* **Default**: 100 requests per hour per API key
* **Exceeding Limit Response**:

```json
{
  "detail": "Too Many Requests"
}
```

Status Code: `429 TOO MANY REQUESTS`

---

## ğŸ§ª Testing

Run all tests:

```bash
pytest
```

Run specific test files:

```bash
pytest app/tests/test_crawler.py -v
pytest app/tests/test_book_api.py -v
pytest app/tests/test_change_book_api.py -v
pytest app/tests/test_schedular.py -v
```

Test coverage:

```bash
pytest --cov=app --cov-report=html
```

Open report:

```bash
open htmlcov/index.html
```

---

## ğŸ³ Deployment

### Docker Compose (Full Stack)

```bash
docker-compose up --build
```

Services started:

* MongoDB
* Redis
* FastAPI (port 8000)
* Scheduler

### Production Environment Variables

```env
MONGO_URI=mongodb://mongo:27017
REDIS_URL=redis://redis:6379
HOST=https://your-domain.com
ADMIN_API_KEY=<strong-random-key>
```

### Manual Deployment (VM/Cloud)

Example `systemd` service:

```ini
[Unit]
Description=Book Crawler API
After=network.target

[Service]
Type=simple
User=www-data
WorkingDirectory=/opt/book_crawler_api
Environment="PATH=/opt/book_crawler_api/.venv/bin"
ExecStart=/opt/book_crawler_api/.venv/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000
Restart=always

[Install]
WantedBy=multi-user.target
```

Enable and start service:

```bash
sudo systemctl enable book-api
sudo systemctl start book-api
```

---

## â— Troubleshooting

* **ModuleNotFoundError: No module named 'app'**
  Run from project root.

* **MongoDB Connection Error**
  Ensure MongoDB is running: `docker-compose ps`

* **Redis Connection Error**
  Test Redis: `docker exec -it book_crawler_api_redis_1 redis-cli PING`

* **Scheduler Not Detecting Changes**
  Check if scheduler is running: `ps aux | grep scheduler`
  Trigger manually: `python -m app.scheduler.detector`

---

## ğŸ“ Support & License

* **Email**: [sudipto@filerskeepers.co](mailto:sudipto@filerskeepers.co)
* **GitHub Issues**: [Open Issue](https://github.com/yourusername/book_crawler_api/issues)
* **License**: MIT - see [LICENSE](LICENSE) file

---

## ğŸ™ Acknowledgments

* [Books to Scrape](https://books.toscrape.com)
* [FastAPI](https://fastapi.tiangolo.com/)
* [MongoDB](https://www.mongodb.com/)
* [Selectolax](https://github.com/rushter/selectolax)

---

**Happy Crawling! ğŸš€**

```

---

If you want, I can also **create a shorter, â€œquick-startâ€ version** suitable for GitHub so users can set up and run the API in **under 5 minutes**, without going through the full detailed guide. This is great for attracting more users.  

Do you want me to do that?
```
